{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "try:\n",
    "    import multiprocess as mp\n",
    "except:\n",
    "    import multiprocessing as mp\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from helper import load_data, data_source_release\n",
    "from helper import preprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.style as style\n",
    "style.use('seaborn-poster') #sets the size of the charts\n",
    "style.use('ggplot')\n",
    "\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.linewidth\"] = 1\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 1\n",
    "plt.rcParams['grid.color'] = \"#cccccc\"\n",
    "# plt.rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadRuntimeDF(file_name):\n",
    "    return pd.read_pickle('../export/dfs/' + file_name + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphMetricsDouble(df1, df2, x_data_type, x_label, save=None):\n",
    "    # Convert num samples to categorical for even spacing\n",
    "    lb_ticks = list(set(df1[x_data_type].tolist()))\n",
    "    lb_ticks.sort()\n",
    "    conversion_map = {lb_ticks[i]: i for i in range(len(lb_ticks))}\n",
    "    df1[x_data_type] = df1[x_data_type].map(conversion_map)\n",
    "    df2[x_data_type] = df2[x_data_type].map(conversion_map)\n",
    "    \n",
    "    df1 = df1.rename({'LLH': 'Distribution'}, axis='columns')\n",
    "    df2 = df2.rename({'LLH': 'Distribution'}, axis='columns')\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(22, 10))\n",
    "    sns.lineplot(x=x_data_type, y=\"NLLH\", hue=\"Distribution\", style=\"Model\", data=df1, ax=axs[0][0])\n",
    "    sns.lineplot(x=x_data_type, y=\"KLD\", hue=\"Distribution\", style=\"Model\", data=df1, ax=axs[0][1])\n",
    "    sns.lineplot(x=x_data_type, y=\"D-KS\", hue=\"Distribution\", style=\"Model\", data=df1, ax=axs[0][2])\n",
    "    sns.lineplot(x=x_data_type, y=\"Mass\", hue=\"Distribution\", style=\"Model\", data=df1, ax=axs[0][3])\n",
    "    \n",
    "    sns.lineplot(x=x_data_type, y=\"NLLH\", hue=\"Distribution\", style=\"Model\", data=df2, ax=axs[1][0])\n",
    "    sns.lineplot(x=x_data_type, y=\"KLD\", hue=\"Distribution\", style=\"Model\", data=df2, ax=axs[1][1])\n",
    "    sns.lineplot(x=x_data_type, y=\"D-KS\", hue=\"Distribution\", style=\"Model\", data=df2, ax=axs[1][2])\n",
    "    sns.lineplot(x=x_data_type, y=\"Mass\", hue=\"Distribution\", style=\"Model\", data=df2, ax=axs[1][3])\n",
    "    handles, labels = axs[0][1].get_legend_handles_labels()\n",
    "    \n",
    "    x_strings=[\"{}\".format(x) for x in lb_ticks]\n",
    "    for ax in plt.gcf().get_axes():\n",
    "        ax.get_legend().remove()\n",
    "        ax.set_xticks(range(len(x_strings)))\n",
    "        ax.set_xticklabels(x_strings)\n",
    "        ax.tick_params(axis='both', labelsize=16)\n",
    "        ax.xaxis.label.set_size(16)\n",
    "        ax.yaxis.label.set_size(14)\n",
    "        ax.set_ylabel('')  \n",
    "        ax.set_xlabel(x_label) \n",
    "    \n",
    "    pad = 5\n",
    "    objs = []\n",
    "    scenarios = ['Clasp-factoring', 'LPG-Zenotravel']\n",
    "    for ax, txt in zip(axs[:,0], scenarios):\n",
    "        tmp = ax.annotate(txt, xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                fontsize=16, ha='right', va='center', rotation=90, fontweight='bold')\n",
    "        objs.append(tmp)\n",
    "    titles = ['Negative Log Likelihood', 'KL Divergence', 'KS Distance', 'Density Area Outside [0,1.5*MAX(T)]']\n",
    "    for ax, txt in zip(axs[0], titles):\n",
    "        tmp = ax.annotate(txt, xy=(0.5, 1), xytext=(0, pad),\n",
    "                    xycoords='axes fraction', textcoords='offset points',\n",
    "                    fontsize=16, ha='center', va='baseline', fontweight='bold')\n",
    "        objs.append(tmp)\n",
    "    fig.subplots_adjust(top=0.85, left=0.15, right=0.85, bottom=0.15)\n",
    "    lgd = fig.legend(handles, labels, loc='lower center', ncol=len(labels), prop={'size': 18}, frameon=True)\n",
    "    lgd.get_frame().set_edgecolor('black')\n",
    "    objs.append(lgd)\n",
    "    fig.tight_layout(rect=[0.2, 0.05, 1.1, 0.9])\n",
    "    fig.show()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig('../export/images/' + save + '.svg', format='svg', dpi=1200, bbox_extra_artists=objs, bbox_inches='tight')\n",
    "        \n",
    "def graphMetricsSingleLBDouble(raw_df1, raw_df2, lb, save=None):\n",
    "    df1 = raw_df1.copy()\n",
    "    df1 = df1[df1['LB'] == lb]\n",
    "    df2 = raw_df2.copy()\n",
    "    df2 = df2[df2['LB'] == lb]\n",
    "    graphMetricsDouble(df1, df2, 'Num Samples', 'Samples per Instance', save)\n",
    "    \n",
    "def graphMetricsSingleNumSamplesDouble(raw_df1, raw_df2, num_samps, save=None):\n",
    "    df1 = raw_df1.copy()\n",
    "    df1 = df1[df1['Num Samples'] == num_samps]\n",
    "    df2 = raw_df2.copy()\n",
    "    df2 = df2[df2['Num Samples'] == num_samps]\n",
    "    graphMetricsDouble(df1, df2, 'LB', 'Percent of Data Censored', save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clasp = loadRuntimeDF('clasp-factoring_new')\n",
    "df_zeno = loadRuntimeDF('lpg-zeno_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clasp['KLD'] = df_clasp['KLD'].str[0].astype(float)\n",
    "df_clasp['Model'].replace('BayesianDistnet', 'Bayes DistNet', inplace=True)\n",
    "df_clasp['Model'].replace('Distnet', 'DistNet', inplace=True)\n",
    "df_zeno['KLD'] = df_zeno['KLD'].str[0].astype(float)\n",
    "df_zeno['Model'].replace('BayesianDistnet', 'Bayes DistNet', inplace=True)\n",
    "df_zeno['Model'].replace('Distnet', 'DistNet', inplace=True)\n",
    "consolidatedDf = df_clasp.groupby(['Num Samples', 'LB', 'Model', 'LLH'], as_index=False).mean()\n",
    "consolidatedDf = consolidatedDf.set_index(['Num Samples', 'LB', 'Model', 'LLH'])\n",
    "consolidatedDf\n",
    "# df.groupby('Model').mean().reindex(models).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which graph to create\n",
    "# graphMetricsSingleLBDouble(df_clasp, df_zeno, 0, save=\"experiment_num_samples_0\")\n",
    "# graphMetricsSingleNumSamplesDouble(df_clasp, df_zeno, 8, save=\"experiment_lb_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_palette = sns.color_palette()\n",
    "sns.palplot(current_palette)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
